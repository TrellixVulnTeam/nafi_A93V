
import re, os
from re import RegexFlag

import multiprocessing

from abc import ABCMeta
from abc import abstractmethod

import sqlite3
from contextlib import contextmanager

import subprocess

from nafi.utils import natural_keys
from nafi.utils import LogEngine
from nafi.utils import benchmark
from nafi.utils import Globals

from nafi.exceptions import workflowException


class workflowManager:
    """ Class responsable for creating the SQLite database and its tables
        and managing all records generated by the workflow.
    """

    def __init__(self, wfname=None):

        self.wfname = wfname
        if self.wfname is not None:
            self.rootdir = self.getRootDirectory()
            self.createProcessTable()

        self.wfid = None

        return

    def getRootDirectory(self):
        """ Return the directory where the workflow database
            is created. By default it will be under '~\Documents\nafi\workflows\databases'
        """
        if Globals.WORKFLOWS_BASEDIR[0] == '~':
            return os.path.expanduser(Globals.WORKFLOWS_BASEDIR)
        else:
            return os.path.join('', Globals.WORKFLOWS_BASEDIR)

    @contextmanager
    def getConnection(self):
        """ Manage the connection with the SQLite database and enable
            foreign keys support. The connection is wrapped within a
            context manager generator
        """

        try:
            if os.path.exists(self.rootdir) is False:
                os.makedirs(self.rootdir)
            db_name = os.path.join(self.rootdir, '{0}.db'.format(self.wfname))
            conn = sqlite3.connect(db_name)

            # Enable foreign key support for database
            cur = conn.cursor()
            cur.execute('pragma foreign_keys = on;')

            yield conn

        except Exception as error:
            conn.rollback()
            raise workflowException('Workflow Database Error: {0}'.format(repr(error)))

        else:
            #print('commit transx')
            conn.commit()

        return conn

    def createProcessTable(self):
        """ Create the tables used by the workflow process management
        """

        with self.getConnection() as conn:
            try:
                cur = conn.cursor()
                cur.execute("""\
                                CREATE TABLE IF NOT EXISTS workflows
                                (
                                    wfid integer primary key autoincrement,
                                    name varchar(100) not null,
                                    rootdir varchar(300) not null,
                                    desc varchar(100),
                                    unique(name, rootdir)
                                );""")

                cur.execute("""\
                                CREATE TABLE IF NOT EXISTS process_run
                                (
                                    ID INTEGER PRIMARY KEY AUTOINCREMENT,
                                    pUID INTEGER, Desc VARCHAR(200) NOT NULL,
                                    PATH CHAR(3),
                                    ROW CHAR(3),
                                    Acqdate VARCHAR(10),
                                    fk_wfid integer NOT NULL,
                                    foreign key(fk_wfid) references workflows(wfid)
                                    on update cascade on delete cascade
                                );""")
                cur.close()

            except sqlite3.OperationalError:
                cur.close()

            except sqlite3.Error:
                cur.close()
                raise workflowException('Error creating table database \'process_run\'')
        return


    def initWorkflow(self, wf_name, working):

        with self.getConnection() as conn:
            try:
                cur = conn.cursor()

                if self.wfid is None:
                    data = cur.execute("""\
                                           select wfid from workflows where name=? and rootdir=? """, (wf_name, working)).fetchone()

                    if data is None:
                        # we need to insert a new record in the 'workflow' table
                        cur.execute("""\
                                        insert into workflows (name, rootdir, desc) 
                                        values (?, ?, ?);""", (wf_name, working, 'Write description, if needed'))

                    data = cur.execute("""\
                                           select wfid from workflows where name=? and rootdir=? """, (wf_name, working)).fetchone()

                    if data is not None:
                        self.wfid = data[0]

            except sqlite3.Error as error:
                cur.close()
                raise workflowException('Database {0}: {1}'.format(self.wfname, repr(error)))

        return

    def logStep(self, pid, desc, landsatScene):
        """ Create a database record in the 'process_run' table.
            The record contains the processing step UID, the processing step
            description and the scene parameters (path, row and acquisition date)
        """

        with self.getConnection() as conn:
            try:
                cur = conn.cursor()
                cur.execute("""\
                                insert into process_run ('pUID', 'Desc', 'PATH', 'ROW', 'Acqdate', 'fk_wfid')
                                values (?, ?, ?, ?, ?, ?)""", (pid, desc, landsatScene.path, landsatScene.row, landsatScene.acqdate, self.wfid))
                cur.close()

            except sqlite3.Error as error:
                cur.close()
                raise workflowException('Error accessing database: {0}'.format(repr(error)))
        return

    def getProcessSteps(self, landsatScene):
        """ Return all processing steps associated with the scene
            'landsatScene' object
        """

        with self.getConnection() as conn:
            try:
                cur = conn.cursor()
                data = cur.execute("""\
                                        select pUID, Desc from process_run where PATH=? and ROW=? and Acqdate=? 
                                        and fk_wfid=?""", (landsatScene.path, landsatScene.row, landsatScene.acqdate, self.wfid)).fetchall()

                if len(data) == 0:
                    return ()
                else:
                    pids, descriptions = list(zip(*data))

            except sqlite3.Error as error:
                cur.close()
                raise workflowException('Database {0}: {1}'.format(self.wfname, repr(error)))

        return pids

    def deleteDownSteps(self, landsatScene, pid):
        """ Delete from the database for the scene object 'landsatScene', all the processing steps with a
            process unique identifier >= 'puid'. We assume that the computation steps are not independent,
            e.i. if a problem occurred at step 10 for example, every thing downstream could be garbage (step 20, 30, 40 etc.)
        """

        with self.getConnection() as conn:
            try:
                cur = conn.cursor()
                cur.execute("""\
                                delete from process_run where PATH=? and ROW=? and Acqdate=? 
                                and fk_wfid=? and pUID >= ?""", (landsatScene.path, landsatScene.row, landsatScene.acqdate, self.wfid, pid))

            except sqlite3.Error as error:
                cur.close()
                raise workflowException('Database {0}: {1}'.format(self.wfname, repr(error)))
        return

    def deleteAllSteps(self, landsatScene):
        """ Delete all records in the table 'process_run' for the
            scene under consideration (landsatScene.path, landsatScene.row
            and landsatScene.acqdate)
        """

        with self.getConnection() as conn:
            try:
                cur = conn.cursor()
                cur.execute("""\
                                delete from process_run where PATH=? and ROW=? and Acqdate=? 
                                and fk_wfid=?""", (landsatScene.path, landsatScene.row, landsatScene.acqdate, self.wfid))

            except sqlite3.Error as error:
                cur.close()
                raise workflowException('Database {0}: {1}'.format(self.wfname, repr(error)))
        return

    def deleteAllRecords(self):
        """ Execute the SQL 'delete from process_run'
        """

        with self.getConnection() as conn:
            try:
                cur = conn.cursor()
                cur.execute("delete from process_run")

            except sqlite3.Error as error:
                cur.close()
                raise workflowException('Database {0}: {1}'.format(self.wfname, repr(error)))
        return


class baseWF(metaclass=ABCMeta):
    """ Workflow base class. All workflow implementations must be derived from this class.
        The 'baseWF' class has one abstract method 'processScene' and one static method
        'fileNamingConvention' that must be present in the derived class. The base class takes care
        of a lot of background tasks like tar file decompresson, database management, processing step
        logging and retrieval etc....
    """

    def __init__(self, config_lk):

        # initialized from a derived class
        self.wf_name = None
        self.dbase = None

        # Initialize logger
        self.logger = LogEngine().logger

        self.config = config_lk
        self.force = config_lk['force']

        self.scene = None
        self.pids = None
        self.p_uid = 0

        # Init SAGA command line
        self.__initSAGA()

        return

    @abstractmethod
    def processScene(self, scene):
        pass


    @staticmethod
    def fileNamingConvention():
        pass


    def __initSAGA(self):

        self.saga_cmd = self.config['saga_cmd']

        # Modify 'saga_cmd' according to script options (verbose mode, number of cores)
        cores = self.config['saga_cores']
        if cores > multiprocessing.cpu_count():
            cores = multiprocessing.cpu_count()

        self.saga_cmd = self.saga_cmd + ' -c={0}'.format(cores)

        if self.config['saga_verbose'] is False:
            self.saga_cmd = self.saga_cmd + ' -f={0}'.format('s')

        return


    def setInitialProcessUID(self, puid):
        """ Step the initial process unique identifer (puid) for a workflow
            section. Until the puid is reset with this function, every call to
            a new workflow step will increment the workflow puid counter (self.p_uid)
            by 10 (kind of an arbitrary increment counter)
        """

        self.p_uid = puid
        return

    @benchmark
    def createExtractedBandList(self):
        """ extract band files from the tar archive and return an ordered list
            of all band file names. As of now, this 'processing' step is never
            logged into the database, and will be executed every time the workflow
            is run
        """

        L8_bands = None

        # Working directory for extracted L8 bands
        working_dir = self.config['working_d']
        outpath_bands = self.scene.extractBands(working_dir)

        if outpath_bands is not None:
            # Create a list all filenames extracted from the downloaded tar file
            f_Bands = os.listdir(outpath_bands)

            # Search only for filename ending in '_Bx.TIF' and add them to the list of L8 band files
            L8_bands = [x for x in f_Bands if re.search(r'_B(\d+)\.TIF$', x, flags=RegexFlag.IGNORECASE)]
            L8_bands.sort(key=natural_keys)

            if len(L8_bands) != self.scene.getNumberOfBands():
                self.logger.critical('Skipping scene: Path/Row= [%s/%s] date= [%s]', self.scene.path, self.scene.row, self.scene.acqdate)
                raise workflowException('Missing band files detected: {0} found instead of {1}'.format(len(L8_bands), '11'))
        else:
            L8_bands = None
            self.logger.critical('Error decompressing %s', str(self.scene))
            raise workflowException('Error passing original band files')

        filename = L8_bands[0]
        title, info1, info2, info3 = self.scene.decodeProduct(filename)

        # log band file information
        self.logger.info(' ')
        self.logger.info('%s', title)
        self.logger.info('%s', info1)
        self.logger.info('%s', info2)
        self.logger.info('%s', info3)
        self.logger.info(' ')

        return L8_bands

    @benchmark
    def executeSAGATool(self, tool_cmd, f_out, desc=''):
        """ This function executes the SAGA command 'tool_cmd', check if the
            output image file has been created 'f_out' (the only way to verify if
            the SAGA command executed successfully). The function also log the
            processing step in the workflow database and autoincrement the process
            step UID.
        """

        if self.runSep(self.p_uid):

            self.logger.debug('[SAGA cmd]: %s', tool_cmd)
            subprocess.call(self.saga_cmd + tool_cmd)

            # if output file exist log workflow step
            if os.path.isfile(f_out):
                if not desc:
                    desc = '{0}: Processing step {1}'.format(self.wf_name, self.p_uid)

                self.logWorkflowStep(self.p_uid, desc)
            else:
                raise workflowException('SAGA process \'{0}\' output file is missing: {1}'.format(desc, f_out))
        else:
            self.logger.info(desc + ': done.')

        # increment p_uid
        self.p_uid += 10

        return

    # This function create the SQLite database
    # It can not be overridden in a derived workflow class (double underscore)

    def initDatabase(self):
        """ Create the SQLite database named after the workflow class name
            The actual work is delegated to the database manager class workflowManager
        """

        try:
            self.dbase = workflowManager(self.wf_name)

            working = self.config['working_d']
            self.dbase.initWorkflow(self.wf_name, working)

        except sqlite3.Error as error:
            self.logger.warning('Database %s: %s', self.wf_name, error.args)
            #print('Database %s: %s', self.wf_name, error.args)

        return


    def logWorkflowStep(self, pid, desc):
        """ Logs a processing step with unique process identifier pid
            in the workflow database
        """

        self.dbase.logStep(pid, desc, self.scene)
        return

    def getWorkflowSteps(self):
        """ Return all the processing steps associated with the current
            scene present in the workflow database
        """

        return self.dbase.getProcessSteps(self.scene)

    def deleteDownSteps(self, puid):
        """ Delete from the database, only the processing steps with a
            process unique identifier >= 'puid'. It assumes that the computation
            steps are not independent, e.i. if a problem occurred at step 10 for
            example, every thing downstream could be garbage (step 20, 30, 40 etc.)
        """

        self.dbase.deleteDownSteps(self.scene, puid)
        return

    def deleteAllSteps(self):
        """ Delete from the database, all processing steps associated
            with the current scene (self.scene)
        """

        self.dbase.deleteAllSteps(self.scene)
        return


    def runSep(self, puid):
        """ The function checks if the processing step needs to be run
            It returns 'False' if the processing step exists in the database
            and need to be skipped, or True if the computation needs to be run,
            either for the first time or if the flag force rerun is True.
            To be on the safe side, the function assumes that the workflow processing
            steps are not indepent. If a missing step is detected, all the subsequent steps
            are delete from the database.
        """

        bAction = False
        # If forced rerun is set (-f option) 'runStep' always return 'True' i.e. the processing step will be executed.
        # The database entries have already been deleted in the 'processScene' function called earler
        if self.force:
            bAction = True
        else:
            # Let's load from the database which steps have been already completed
            # Get the workflow step 'pid' list, if it already exist
            self.pids = self.getWorkflowSteps()

            if puid in self.pids:
                # if current pid is in the list and forced rerun is not enabled
                # Do not execute the processing step
                bAction = False
            else:
                # the pid is not in the list. Execute the processing step
                # and assume all downstream steps are stale and delete them
                self.deleteDownSteps(puid)
                bAction = True

        return bAction
